This is how i am getting it in response

Response JSON:
{
    "jsonrpc": "2.0",
    "id": 1,
    "result": {
        "combined_summary": "SUBMISSION SUMMARY\nClient: High Tech Solution\nDate: July 15, 2025\n\n------------------------------------------------------------\n1. SUBMISSION COVER LETTER / NARRATIVE\n------------------------------------------------------------\n\u00e2\u009c\u0094 Exposure Description:\n- High Tech Solution is a corporation engaged in IT services and operations.\n- Main operations are conducted at a commercial office space, with activities centered on technology services and the use of advanced hi-tech systems and equipment.\n- The primary insured premises is 123 Penny Lane, Springfield, Illinois, with an additional office at 456 Elm St, Suite 200, Los Angeles, CA 90001. The business owns the Springfield location and operates within city limits.\n- Property details include advanced technological equipment and systems. Supplemental forms suggest possible exposures related to apartment buildings and contractors, indicating ancillary activities or property interests beyond core IT operations.\n\n\u00e2\u009c\u0094 Placement Strategy:\n- The program is placed with Phoenix Insurers Ltd. under Program 1, with coverage bound for a three-year term. Coverage includes Commercial Property and Accounts Receivable/Valuable Papers, with earthquake and flood as specified causes of loss.\n\n\u00e2\u009c\u0094 Renewal Highlights:\n- No explicit renewal changes noted; the submission is marked as bound for the upcoming term.\n\n\u00e2\u009c\u0094 Changes YOY:\n- No specific changes in exposure, values, or operations are indicated compared to prior years.\n\n\u00e2\u009c\u0094 Additional Overview Details:\n- Effective Date: July 15, 2025\n- Submission Type: Bound (New or Renewal not specified)\n- Current Situation: Submission is for a bound policy with coverage confirmed.\n\n------------------------------------------------------------\n2. LOSS HISTORY PACKAGE\n------------------------------------------------------------\n\u00e2\u009c\u0094 5\u00e2\u0080\u009310 Year Loss Runs:\n- Three-year loss history is provided for the Los Angeles office location.\n\n\u00e2\u009c\u0094 Large Loss Details:\n- Four claims reported: theft of office equipment, floor damage from tremors, an electrical fire in a storage room (largest reserve), and window damage from a minor earthquake.\n\n\u00e2\u009c\u0094 Trends Analysis:\n- Losses are concentrated at a single address and reflect exposure to theft, fire, and earthquake-related events. Two claims remain open, with the electrical fire representing the largest outstanding reserve.\n\n\u00e2\u009c\u0094 Summary of Loss History:\n- Status: Losses Reported\n- Summary: Four claims over three years totaling $92,450 incurred, with $23,000 in open reserves.\n- Major Incidents: Electrical fire in storage room (largest reserve).\n\n------------------------------------------------------------\n3. EXPOSURE DATA",
        "individualized_summary": [
            {
                "file_name": "123ASJKDB1JKBSAF_attachment_Acord_125_High_tech_solution.pdf",
                "summary": "The applicant, High Tech Solution, is a corporation located at 456 Elm St, Suite 200, Los Angeles, CA 90001, with primary operations in IT services and operations. The proposed policy is with Phoenix Insurers Ltd. under Program 1, with coverage effective from July 15, 2025, to July 15, 2028, and the transaction is marked as bound. The primary insured premises is 123 Penny Lane, Springfield, Illinois, where the applicant is the owner and operations are conducted inside city limits. Annual revenues are reported at $725,000, and the business has full-time employees (exact number not specified). Relevant lines of business indicated include Commercial Property and Accounts Receivable/Valuable Papers, with several supplemental forms attached, such as Apartment Building and Contractors Supplements, suggesting potential exposures beyond standard IT operations.",
                "document_type": "ACORD",
                "processed_summary": "SUBMISSION SUMMARY\nClient: High Tech Solution\nDate: 07/15/2025\n\n------------------------------------------------------------\n1. SUBMISSION COVER LETTER / NARRATIVE\n------------------------------------------------------------\n\u00e2\u009c\u0094 Exposure Description:\n- High Tech Solution provides IT services and operations.\n- Main operations are conducted at 123 Penny Lane, Springfield, Illinois 62629, with the business mailing address at 456 Elm St, Suite 200, Los Angeles, CA 90001.\n- The Springfield location is owned and situated within city limits.\n- Occupied area is indicated as 1 (unit/area not specified).\n- No specific property construction, age, or protection details are provided.\n- The business employs 1 full-time employee and reports annual revenues of $725,000.\n\n\u00e2\u009c\u0094 Placement Strategy:\n- The submission is for placement with Phoenix Insurers Ltd. under Program 1, facilitated by Prime Brokers Ltd.\n- Lines of business indicated include Commercial Property and Accounts Receivable/Valuable Papers, with supplemental forms for Apartment Building, Contractors, and Installation/Builders Risk attached.\n\n\u00e2\u009c\u0094 Renewal Highlights:\n- The application is marked as a renewal, with the proposed effective date of 07/15/2025 and expiration date of 07/15/2028.\n\n\u00e2\u009c\u0094 Changes YOY:\n- No changes in exposure, values, operations, or loss experience are noted in the provided information.\n\n\u00e2\u009c\u0094 Additional Overview Details:\n- Effective Date: 07/15/2025\n- Submission Type: Renewal\n- Current Situation: Renewal submission for continued coverage; no specific reason for submission stated."
            },
            {
                "file_name": "123ASJKDB1JKBSAF_attachment_Loss_Run_HighTech_Solution.pdf",
                "summary": "Hightech Solution has maintained commercial property coverage over the past three years, with policies consistently in force for their Los Angeles office location. The account has reported four claims during this period, including theft of office equipment, floor damage from tremors, an electrical fire in a storage room, and window damage from a minor earthquake. Total paid losses amount to $69,450, with $23,000 in outstanding reserves, resulting in total incurred losses of $92,450. Two claims remain open, with the largest reserve associated with the electrical fire. The loss history indicates exposure to theft, fire, and earthquake-related events, with claims concentrated at a single address.",
                "document_type": "Loss Run",
                "processed_summary": "LOSS HISTORY PACKAGE\n------------------------------------------------------------\n\u00e2\u009c\u0094 5\u00e2\u0080\u009310 Year Loss Runs:\n- Four claims reported over the past three policy years (2023\u00e2\u0080\u00932025) for commercial property coverage.\n\n\u00e2\u009c\u0094 Large Loss Details:\n- The largest claim is an open electrical fire loss in February 2025, with $30,000 paid and $15,000 reserved, totaling $45,000 incurred.\n\n\u00e2\u009c\u0094 Trends Analysis:\n- Losses include theft, earthquake-related damage, and fire, with moderate frequency and one notably severe incident. Two claims remain open with reserves.\n\n\u00e2\u009c\u0094 Summary of Loss History:\n- Status: Losses Reported\n- Summary: Four claims totaling $92,450 incurred, with a mix of closed and open claims, including fire, theft, and earthquake damage.\n- Major Incidents: Electrical fire in storage room (Feb 2025, $45,000 incurred, open)."
            },
            {
                "file_name": "123ASJKDB1JKBSAF_attachment_SOV.pdf",
                "summary": "The insured is Berkshire Hathaway, with coverage placed through Prime Brokers Pvt. Ltd. and Phoenix Insurers Ltd. The property is a commercial office space located at 456 Elm St, Suite 200, Los Angeles, CA 90001, featuring advanced hi-tech systems and equipment. The schedule indicates 100% values at $35,000,000 on an Actual Cash Value basis. Coinsurance is set at 80%, and requested coverage includes causes of loss such as earthquake and flood. The effective date of coverage is July 15, 2025, and the information is verified and signed by the lead broker.",
                "document_type": "SOV",
                "processed_summary": "EXPOSURE DATA\n------------------------------------------------------------\n\n\u00e2\u009c\u0094 COPE Information (Construction, Occupancy, Protection, Exposure):\n- Commercial office space at 456 Elm St, Suite 200, Los Angeles, CA 90001, utilizing advanced hi-tech systems, equipment, and infrastructure.\n\n\u00e2\u009c\u0094 Statement of Values (SOV):\n- Total Insurable Value (TIV): $35,000,000\n- Building Limit: [Not specified]\n- Contents/BPP Limit: [Not specified]\n- Business Income: [Not specified]\n\n\u00e2\u009c\u0094 Financials:\n- Coinsurance: 80%\n\n\u00e2\u009c\u0094 CAT Worksheets:\n- Earthquake and Flood coverage indicated.\n\n\u00e2\u009c\u0094 Specialty Schedules:\n- Sprinkler leakage and vandalism excluded.\n\n\u00e2\u009c\u0094 Additional Exposure Details:\n- Total Locations: 1\n- Main Operations: Commercial office operations\n- Property Details: Valuation basis is Actual Cash Value (ACV)\n- Deductible Requested: [Not specified]"
            }
        ]
    }
}
(sumvenv) jarvis@Jarvis-agent-vm:~/Musaddique/Ultimate_Summary$ 

remeber going forward i have store this is excel but in proper manner as i said earlier 
filename : whatever the filename is 
summar , etc etc 


summary handler.py




import os
import json
from typing import Dict, Any, List
import pandas as pd

from summary_service import summary_require_env
from summary_storage_utils import download_blob_to_input_folder
from summary_loader import load_summary_document
from summary_classifier import summarize_first_page
from Data_Filler import extract_filenames_and_summaries
from Data_Filler import connect_to_blob, beautify_extracted_data
from newsummary import summarize_attachments
from separate_summary import individual_summary
from Data_Filler import beautify_api_response

INPUT_FOLDER = "input"
OUTPUT_FOLDER = "output"


def clean_result_for_api(result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Build a compact, clean dictionary response from the full 'result' dict.

    Output shape:
    {
      "combined_summary": "<string or None>",
      "individualized_summary": [
         { "file_name": ..., "summary": ..., "document_type": ..., "processed_summary": ... },
         ...
      ]
    }

    This removes raw 'text' and error details from the per-file items.
    """
    combined = result.get("combined_summary")

    raw_individualized = result.get("individualized_summary", [])
    cleaned_list: List[Dict[str, Any]] = []

    if isinstance(raw_individualized, list):
        for item in raw_individualized:
            cleaned_item = {
                "file_name": item.get("file_name"),
                "summary": item.get("summary"),
                "document_type": item.get("document_type"),
                "processed_summary": item.get("processed_summary"),
            }
            cleaned_list.append(cleaned_item)

    return {
        "combined_summary": combined,
        "individualized_summary": cleaned_list,
    }


async def summarize_blob_pdfs_layout(blob_urls: List[str]) -> Dict[str, Any]:
    """
    For each Blob URL:
      - Download PDF into input/
      - Use Form Recognizer to extract text (all pages)
      - Use LLM to summarize the FIRST PAGE

    Saves the full detailed JSON (including raw text) to ./output/layout_summaries_result.json
    but RETURNS a cleaned dictionary suitable for API responses (no raw 'text').
    """
    try:
        summary_require_env()
    except Exception as e:
        # Return a clean, empty-shaped response on environment failure
        return {"combined_summary": None, "individualized_summary": []}

    summaries: List[Dict[str, Any]] = []

    for blob_url in blob_urls:
        file_name = None

        # 1) Download PDF
        try:
            local_pdf = await download_blob_to_input_folder(
                blob_url, input_folder=INPUT_FOLDER
            )
            file_name = os.path.basename(local_pdf)
        except Exception as e:
            summaries.append(
                {
                    "blob_url": blob_url,
                    "file_name": file_name,
                    "summary": None,
                    "text": None,
                    "error": f"Failed to download blob: {e}",
                }
            )
            continue

        # 2) Use Form Recognizer to analyze the document
        try:
            document_content = load_summary_document(local_pdf)
        except Exception as e:
            summaries.append(
                {
                    "blob_url": blob_url,
                    "file_name": file_name,
                    "summary": None,
                    "text": None,
                    "error": f"Form Recognizer failed: {e}",
                }
            )
            continue

        # Extract raw text (stored in first_page_text key by loader)
        text_content = document_content.get("first_page_text", "")

        # 3) Summarize FIRST PAGE using LLM (short summary)
        try:
            summary_text = summarize_first_page(document_content, file_name=file_name)
        except Exception as e:
            summaries.append(
                {
                    "blob_url": blob_url,
                    "file_name": file_name,
                    "summary": None,
                    "text": text_content,
                    "error": f"Summarization failed: {e}",
                }
            )
            continue

        # 4) Success case for this document — include raw text for audit on disk
        summaries.append(
            {
                "blob_url": blob_url,
                "file_name": file_name,
                "summary": summary_text,
                "text": text_content,
                "error": None,
            }
        )

    result: Dict[str, Any] = {"status": True, "summaries": summaries}

    # 5) Save result to ./output (retain 'text' on disk for audit)
    try:
        os.makedirs(OUTPUT_FOLDER, exist_ok=True)
        out_path = os.path.join(OUTPUT_FOLDER, "layout_summaries_result.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
    except Exception as e:
        result["save_error"] = str(e)

    # 6) Beautify and optionally write to Excel (unchanged; uses only file_name + summary)
    try:
        extracted_data = extract_filenames_and_summaries(OUTPUT_FOLDER)
        print("Beautified Extracted Data:\n")
        beautify_data = beautify_extracted_data(extracted_data)
        print(beautify_data)

        DATA_URL = os.getenv("Data_URL")
        if DATA_URL:
            try:
                updated_data = connect_to_blob(DATA_URL, beautify_data)
                print("Blob Data Processed Successfully.")
                if isinstance(updated_data, pd.DataFrame):
                    print("Blob Data Processed Successfully and Excel Updated.")
                else:
                    # updated_data may be an error dict
                    print(f"Error: {updated_data.get('error')}")
            except Exception as e:
                print(f"Error processing blob data: {e}")
        else:
            print("Data_URL environment variable is not set.")
    except Exception as e:
        print(f"Error in beautify / blob step: {e}")

    # 7) Combined submission summary (uses file_name + summary only)
    combined_summary_text = None
    filenamesummary_extracted_beautified: List[Dict[str, Any]] = []
    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        output_folder = "output"

        json_path = os.path.join(base_dir, output_folder, "layout_summaries_result.json")
        with open(json_path, "r", encoding="utf-8") as f:
            layout_json = json.load(f)

        filenamesummary_extracted_beautified = extract_filenames_and_summaries(output_folder)
        print("Per-document summaries (with text):")
        print(filenamesummary_extracted_beautified)

        combined_summary_text = summarize_attachments(filenamesummary_extracted_beautified)
        print("Combined Submission Summary:")
        print(combined_summary_text)

        result["combined_summary"] = combined_summary_text
    except Exception as e:
        print(f"Error generating combined submission summary: {e}")
        result["combined_summary_error"] = str(e)
        combined_summary_text = None

    # 8) Individual typed summaries (ACORD / Loss Run / SOV)
    try:
        # individual_summary expects attachments each having 'file_name','summary','text'
        individualized_summary = individual_summary(filenamesummary_extracted_beautified)
        print("Individualized Summary Output (raw):")
        print(individualized_summary)
        result["individualized_summary"] = individualized_summary
    except Exception as e:
        print(f"Error generating Individual summary: {e}")
        result["individualized_summary_error"] = str(e)
        result["individualized_summary"] = []


    # 9) CLEAN the result for the API and return only the compact payload (no raw text)
    cleaned_payload = clean_result_for_api(result)


    
    try:
        beautified = beautify_api_response(cleaned_payload)
        print(beautified)
        # print("\n----- COMBINED SUBMISSION SUMMARY (beautified) -----\n")
        # if beautified["combined_summary"]:
        #     print(beautified["combined_summary"])
        # else:
        #     print("No combined summary available.")

    #     DATA_URL = os.getenv("Data_URL")
    #     if DATA_URL:
    #         try:
    #             updated_data = connect_to_blob(DATA_URL, beautify_data)
    #             print("Blob Data Processed Successfully.")
    #             if isinstance(updated_data, pd.DataFrame):
    #                 print("Blob Data Processed Successfully and Excel Updated.")
    #             else:
    #                 # updated_data may be an error dict
    #                 print(f"Error: {updated_data.get('error')}")
    #         except Exception as e:
    #             print(f"Error processing blob data: {e}")
    #     else:
    #         print("Data_URL environment variable is not set.")
    except Exception as e:
        print(f"Error in beautify / blob step: {e}")



    # return cleaned_payload
    return beautified




Data filler.py



import os
import json
import pandas as pd
from io import BytesIO
from azure.storage.blob import BlobServiceClient
from typing import List, Dict, Any
from summary_service import summary_require_env
from summary_service import AZURE_STORAGE_CONNECTION_STRING, _parse_blob_url
import re
from typing import Iterable


def extract_filenames_and_summaries(output_folder: str) -> List[Dict[str, Any]]:
    """
    Extracts file names, summaries, and raw text from the JSON file in the output folder.

    Returns list of:
      {
        "file_name": "<file_name>",
        "summary": "<summary>",
        "text": "<full_text_or_first_page_text>"
      }
    """
    result_file = os.path.join(output_folder, "layout_summaries_result.json")

    if not os.path.exists(result_file):
        raise FileNotFoundError(f"No JSON result file found at {result_file}")

    with open(result_file, "r", encoding="utf-8") as f:
        data = json.load(f) 

    summaries = data.get("summaries", [])

    extracted_data = [
        {
            "file_name": item["file_name"],
            "summary": item["summary"],
            "text": item.get("text", "")  # <-- thisismyNEW: raw text stored here
        }
        for item in summaries
        if "file_name" in item and "summary" in item
    ]

    return extracted_data

#Absolute working code 
def beautify_extracted_data(extracted_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Beautifies and prints the extracted data in the desired format.
    NOTE: we do NOT include 'text' here, only file_name + summary + reference_number.
    """
    beautified_data: List[Dict[str, Any]] = []

    for item in extracted_data:
        file_name = item["file_name"]
        reference_number = file_name.split("_")[0]

        beautified_item = {
            "reference_number": reference_number,
            "file_name": file_name,
            "summary": item["summary"],
        }
        beautified_data.append(beautified_item)

        print(f"file_name: {file_name}")
        print(f"reference_number: {reference_number}")
        print(f"summary: {item['summary']}\n")

    return beautified_data



def connect_to_blob(blob_url: str, beautified_data: List[Dict[str, Any]]):
    """
    Update existing rows in the Excel blob by concatenating file_name + summary
    into the Summary cell for matching 'Unique refrence Number'.
    Does NOT create new rows if there is no match.
    """
    try:
        summary_require_env()
    except Exception as e:
        return {"status": False, "error": f"Environment misconfigured: {e}"}

    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("AZURE_STORAGE_CONNECTION_STRING not set")

    try:
        container, blob_path = _parse_blob_url(blob_url)
    except Exception as e:
        return {"status": False, "error": f"Unable to parse blob URL: {e}"}

    try:
        blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)
        blob_client = blob_service_client.get_blob_client(container=container, blob=blob_path)

        blob_data = blob_client.download_blob().readall()
        excel_data = pd.read_excel(BytesIO(blob_data))

        if "Unique refrence Number" not in excel_data.columns:
            return {"status": False, "error": "'Unique refrence Number' column is missing in the Excel file."}

        if "Summary" not in excel_data.columns:
            excel_data["Summary"] = None

        for item in beautified_data:
            reference_number = item.get("reference_number")
            file_name = item.get("file_name", "")
            summary_text = item.get("summary", "")

            matches = excel_data[excel_data["Unique refrence Number"] == reference_number].index

            if matches.empty:
                print(f"Reference number not found in Excel, skipping: {reference_number}")
                continue

            new_fragment = f"file_name: {file_name}\nsummary: {summary_text}"

            for idx in matches:
                existing = excel_data.at[idx, "Summary"]
                if existing is not None and not pd.isna(existing):
                    updated = f"{existing}\n\n{new_fragment}"
                else:
                    updated = new_fragment
                excel_data.at[idx, "Summary"] = updated

        output_stream = BytesIO()
        excel_data.to_excel(output_stream, index=False, engine="openpyxl")
        output_stream.seek(0)
        blob_client.upload_blob(output_stream, overwrite=True)

        return excel_data

    except Exception as e:
        return {"status": False, "error": f"Failed to process blob: {e}"}






def normalize_text(s: str) -> str:
    """
    Clean and normalize an LLM-produced string:
    - Replace escaped unicode sequences like '\\u2714' with '✔' (or remove)
    - Replace repeated newlines with single newline
    - Trim leading/trailing whitespace on each line and collapse multiple spaces
    - Remove stray carriage returns
    """
    if not s:
        return s
    # if the string contains literal backslash-u (e.g. "\\u2714") turn into the actual char
    try:
        # decode unicode-escapes (only when safe) — but avoid double-decoding valid unicode
        # This will transform "\\u2714" into "✔"
        s = s.encode("utf-8").decode("unicode_escape")
    except Exception:
        # fallback: ignore decoding errors
        pass

    # Convert any ✔ variants (some LLMs produce unicode check) to a single visible symbol
    s = s.replace("\u2714", "✔")
    s = s.replace("\\u2714", "✔")  # literal backslash-u

    # Normalize line endings & whitespace
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    # collapse multiple blank lines to single
    s = re.sub(r"\n\s*\n+", "\n\n", s)
    # trim trailing spaces on each line
    s = "\n".join([ln.strip() for ln in s.split("\n")])
    # collapse more-than-two spaces
    s = re.sub(r"[ ]{2,}", " ", s)
    # final strip
    return s.strip()




def beautify_api_response(cleaned_payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    Take cleaned_payload (the dict you return to API) and produce a
    more human-friendly 'beautified' structure for display or Excel upload.

    Input shape:
      {
        "combined_summary": "<string>",
        "individualized_summary": [
          {"file_name": ..., "summary": ..., "document_type": ..., "processed_summary": ...},
          ...
        ]
      }

    Output shape (dict):
      {
        "combined_summary": "<normalized multi-line string>",
        "individualized_summary": [
           {
             "file_name": "<filename>",
             "summary": "<normalized single-paragraph summary>",
             "document_type": "<ACORD|Loss Run|SOV>",
             "processed_summary": "<normalized multi-line narrative>"
           },
           ...
        ]
      }

    This function **does not** return 'text' or raw LLM outputs with \n escapes.
    """
    out = {"combined_summary": None, "individualized_summary": []}

    combined = cleaned_payload.get("combined_summary")
    out["combined_summary"] = normalize_text(combined) if combined else None

    indiv = cleaned_payload.get("individualized_summary", [])
    beautified_list = []
    if isinstance(indiv, Iterable):
        for item in indiv:
            # defensive: ensure keys exist
            file_name = item.get("file_name")
            summary = item.get("summary")
            doc_type = item.get("document_type")
            processed = item.get("processed_summary")

            beautified_item = {
                "file_name": file_name,
                "summary": normalize_text(summary) if summary else None,
                "document_type": doc_type,
                "processed_summary": normalize_text(processed) if processed else None,
            }
            beautified_list.append(beautified_item)

    out["individualized_summary"] = beautified_list
    return out
