Great — understood. I fixed the main problem (mojibake / escaped-unicode like \u00e2\u009c\u0094 and raw \n sequences), and produced two updated files you can copy-paste:
	•	summary_handler.py — unchanged logic flow, but now calls a robust beautifier and returns the cleaned + beautified dict (no raw text). It prints a friendly view and returns the beautified dict (ready for storing to Excel later).
	•	Data_Filler.py — only small tweaks: kept extract_filenames_and_summaries() and beautify_extracted_data() as before, but moved the robust normalize_text() and beautify_api_response() into this file (so you can re-use them if desired). I implemented a robust fix for mojibake / double-encoded UTF-8 sequences and fallbacks.

Copy these two files exactly (replace your existing ones). After that, run your test again — the JSON result will contain human-readable strings (actual line breaks and check marks) and individualized_summary entries without text.

⸻

summary_handler.py (FULL file)

import os
import json
from typing import Dict, Any, List
import pandas as pd

from summary_service import summary_require_env
from summary_storage_utils import download_blob_to_input_folder
from summary_loader import load_summary_document
from summary_classifier import summarize_first_page
from Data_Filler import extract_filenames_and_summaries
from Data_Filler import connect_to_blob, beautify_extracted_data
from newsummary import summarize_attachments
from separate_summary import individual_summary

# import the beautifier from Data_Filler (we put it there)
from Data_Filler import beautify_api_response

INPUT_FOLDER = "input"
OUTPUT_FOLDER = "output"


def clean_result_for_api(result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Build a compact, clean dictionary response from the full 'result' dict.

    Output shape:
    {
      "combined_summary": "<string or None>",
      "individualized_summary": [
         { "file_name": ..., "summary": ..., "document_type": ..., "processed_summary": ... },
         ...
      ]
    }

    This removes raw 'text' and error details from the per-file items.
    """
    combined = result.get("combined_summary")

    raw_individualized = result.get("individualized_summary", [])
    cleaned_list: List[Dict[str, Any]] = []

    if isinstance(raw_individualized, list):
        for item in raw_individualized:
            cleaned_item = {
                "file_name": item.get("file_name"),
                "summary": item.get("summary"),
                "document_type": item.get("document_type"),
                "processed_summary": item.get("processed_summary"),
            }
            cleaned_list.append(cleaned_item)

    return {
        "combined_summary": combined,
        "individualized_summary": cleaned_list,
    }


async def summarize_blob_pdfs_layout(blob_urls: List[str]) -> Dict[str, Any]:
    """
    For each Blob URL:
      - Download PDF into input/
      - Use Form Recognizer to extract text (all pages)
      - Use LLM to summarize the FIRST PAGE

    Saves the full detailed JSON (including raw text) to ./output/layout_summaries_result.json
    but RETURNS a cleaned + beautified dictionary suitable for API responses (no raw 'text').
    """
    try:
        summary_require_env()
    except Exception as e:
        # Return a clean, empty-shaped response on environment failure
        return {"combined_summary": None, "individualized_summary": []}

    summaries: List[Dict[str, Any]] = []

    for blob_url in blob_urls:
        file_name = None

        # 1) Download PDF
        try:
            local_pdf = await download_blob_to_input_folder(
                blob_url, input_folder=INPUT_FOLDER
            )
            file_name = os.path.basename(local_pdf)
        except Exception as e:
            summaries.append(
                {
                    "blob_url": blob_url,
                    "file_name": file_name,
                    "summary": None,
                    "text": None,
                    "error": f"Failed to download blob: {e}",
                }
            )
            continue

        # 2) Use Form Recognizer to analyze the document
        try:
            document_content = load_summary_document(local_pdf)
        except Exception as e:
            summaries.append(
                {
                    "blob_url": blob_url,
                    "file_name": file_name,
                    "summary": None,
                    "text": None,
                    "error": f"Form Recognizer failed: {e}",
                }
            )
            continue

        # Extract raw text (stored in first_page_text key by loader)
        text_content = document_content.get("first_page_text", "")

        # 3) Summarize FIRST PAGE using LLM (short summary)
        try:
            summary_text = summarize_first_page(document_content, file_name=file_name)
        except Exception as e:
            summaries.append(
                {
                    "blob_url": blob_url,
                    "file_name": file_name,
                    "summary": None,
                    "text": text_content,
                    "error": f"Summarization failed: {e}",
                }
            )
            continue

        # 4) Success case for this document — include raw text for audit on disk
        summaries.append(
            {
                "blob_url": blob_url,
                "file_name": file_name,
                "summary": summary_text,
                "text": text_content,
                "error": None,
            }
        )

    result: Dict[str, Any] = {"status": True, "summaries": summaries}

    # 5) Save result to ./output (retain 'text' on disk for audit)
    try:
        os.makedirs(OUTPUT_FOLDER, exist_ok=True)
        out_path = os.path.join(OUTPUT_FOLDER, "layout_summaries_result.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
    except Exception as e:
        result["save_error"] = str(e)

    # 6) Beautify and optionally write to Excel (unchanged; uses only file_name + summary)
    try:
        extracted_data = extract_filenames_and_summaries(OUTPUT_FOLDER)
        print("Beautified Extracted Data:\n")
        beautify_data = beautify_extracted_data(extracted_data)
        print(beautify_data)

        DATA_URL = os.getenv("Data_URL")
        if DATA_URL:
            try:
                updated_data = connect_to_blob(DATA_URL, beautify_data)
                print("Blob Data Processed Successfully.")
                if isinstance(updated_data, pd.DataFrame):
                    print("Blob Data Processed Successfully and Excel Updated.")
                else:
                    # updated_data may be an error dict
                    print(f"Error: {updated_data.get('error')}")
            except Exception as e:
                print(f"Error processing blob data: {e}")
        else:
            print("Data_URL environment variable is not set.")
    except Exception as e:
        print(f"Error in beautify / blob step: {e}")

    # 7) Combined submission summary (uses file_name + summary only)
    combined_summary_text = None
    filenamesummary_extracted_beautified: List[Dict[str, Any]] = []
    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        output_folder = "output"

        json_path = os.path.join(base_dir, output_folder, "layout_summaries_result.json")
        with open(json_path, "r", encoding="utf-8") as f:
            layout_json = json.load(f)

        filenamesummary_extracted_beautified = extract_filenames_and_summaries(output_folder)
        print("Per-document summaries (with text):")
        print(filenamesummary_extracted_beautified)

        combined_summary_text = summarize_attachments(filenamesummary_extracted_beautified)
        print("Combined Submission Summary:")
        print(combined_summary_text)

        result["combined_summary"] = combined_summary_text
    except Exception as e:
        print(f"Error generating combined submission summary: {e}")
        result["combined_summary_error"] = str(e)
        combined_summary_text = None

    # 8) Individual typed summaries (ACORD / Loss Run / SOV)
    try:
        # individual_summary expects attachments each having 'file_name','summary','text'
        individualized_summary = individual_summary(filenamesummary_extracted_beautified)
        print("Individualized Summary Output (raw):")
        print(individualized_summary)
        result["individualized_summary"] = individualized_summary
    except Exception as e:
        print(f"Error generating Individual summary: {e}")
        result["individualized_summary_error"] = str(e)
        result["individualized_summary"] = []

    # 9) CLEAN the result for the API and return only the compact payload (no raw text)
    cleaned_payload = clean_result_for_api(result)

    # 10) Beautify the cleaned payload for display / downstream usage
    try:
        beautified = beautify_api_response(cleaned_payload)

        # print human-friendly combined summary and per-doc items
        print("\n----- COMBINED SUBMISSION SUMMARY (beautified) -----\n")
        if beautified["combined_summary"]:
            print(beautified["combined_summary"])
        else:
            print("No combined summary available.")

        print("\n----- INDIVIDUAL DOCUMENT SUMMARIES (beautified) -----\n")
        for doc in beautified["individualized_summary"]:
            print(f"file_name: {doc.get('file_name')}")
            print(f"document_type: {doc.get('document_type')}")
            print("summary:")
            print(doc.get("summary") or "  -")
            print("processed_summary:")
            print(doc.get("processed_summary") or "  -")
            print("-" * 60)

    except Exception as e:
        print(f"Error in beautify / display step: {e}")
        # fallback to cleaned payload if beautify fails
        beautified = cleaned_payload

    # Return beautified payload (dict). This is what API should return.
    return beautified


⸻

Data_Filler.py (FULL file with robust normalizer & beautifier)

import os
import json
import pandas as pd
from io import BytesIO
from azure.storage.blob import BlobServiceClient
from typing import List, Dict, Any
from summary_service import summary_require_env
from summary_service import AZURE_STORAGE_CONNECTION_STRING, _parse_blob_url
import re
from typing import Iterable

def extract_filenames_and_summaries(output_folder: str) -> List[Dict[str, Any]]:
    """
    Extracts file names, summaries, and raw text from the JSON file in the output folder.

    Returns list of:
      {
        "file_name": "<file_name>",
        "summary": "<summary>",
        "text": "<full_text_or_first_page_text>"
      }
    """
    result_file = os.path.join(output_folder, "layout_summaries_result.json")

    if not os.path.exists(result_file):
        raise FileNotFoundError(f"No JSON result file found at {result_file}")

    with open(result_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    summaries = data.get("summaries", [])

    extracted_data = [
        {
            "file_name": item["file_name"],
            "summary": item["summary"],
            "text": item.get("text", "")  # raw text stored here
        }
        for item in summaries
        if "file_name" in item and "summary" in item
    ]

    return extracted_data


def beautify_extracted_data(extracted_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Beautifies and prints the extracted data in the desired format.
    NOTE: we do NOT include 'text' here, only file_name + summary + reference_number.
    """
    beautified_data: List[Dict[str, Any]] = []

    for item in extracted_data:
        file_name = item["file_name"]
        reference_number = file_name.split("_")[0]

        beautified_item = {
            "reference_number": reference_number,
            "file_name": file_name,
            "summary": item["summary"],
        }
        beautified_data.append(beautified_item)

        print(f"file_name: {file_name}")
        print(f"reference_number: {reference_number}")
        print(f"summary: {item['summary']}\n")

    return beautified_data


def connect_to_blob(blob_url: str, beautified_data: List[Dict[str, Any]]):
    """
    Update existing rows in the Excel blob by concatenating file_name + summary
    into the Summary cell for matching 'Unique refrence Number'.
    Does NOT create new rows if there is no match.
    """
    try:
        summary_require_env()
    except Exception as e:
        return {"status": False, "error": f"Environment misconfigured: {e}"}

    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("AZURE_STORAGE_CONNECTION_STRING not set")

    try:
        container, blob_path = _parse_blob_url(blob_url)
    except Exception as e:
        return {"status": False, "error": f"Unable to parse blob URL: {e}"}

    try:
        blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)
        blob_client = blob_service_client.get_blob_client(container=container, blob=blob_path)

        blob_data = blob_client.download_blob().readall()
        excel_data = pd.read_excel(BytesIO(blob_data))

        if "Unique refrence Number" not in excel_data.columns:
            return {"status": False, "error": "'Unique refrence Number' column is missing in the Excel file."}

        if "Summary" not in excel_data.columns:
            excel_data["Summary"] = None

        for item in beautified_data:
            reference_number = item.get("reference_number")
            file_name = item.get("file_name", "")
            summary_text = item.get("summary", "")

            matches = excel_data[excel_data["Unique refrence Number"] == reference_number].index

            if matches.empty:
                print(f"Reference number not found in Excel, skipping: {reference_number}")
                continue

            new_fragment = f"file_name: {file_name}\nsummary: {summary_text}"

            for idx in matches:
                existing = excel_data.at[idx, "Summary"]
                if existing is not None and not pd.isna(existing):
                    updated = f"{existing}\n\n{new_fragment}"
                else:
                    updated = new_fragment
                excel_data.at[idx, "Summary"] = updated

        output_stream = BytesIO()
        excel_data.to_excel(output_stream, index=False, engine="openpyxl")
        output_stream.seek(0)
        blob_client.upload_blob(output_stream, overwrite=True)

        return excel_data

    except Exception as e:
        return {"status": False, "error": f"Failed to process blob: {e}"}


# -----------------------
# Robust normalizer / beautifier
# -----------------------
def _fix_mojibake(s: str) -> str:
    """
    Try to fix double-encoded UTF-8 sequences like '\\u00e2\\u009c\\u0094'
    which often appear when bytes were UTF-8 but got escaped.

    Strategy (best-effort):
      1) Try unicode-escape decode then latin1->utf-8 decode.
      2) Try latin1->utf-8 directly.
      3) Fallback to original string.
    """
    if not isinstance(s, str) or not s:
        return s
    try:
        # step1: decode unicode-escaped sequences
        step1 = s.encode("utf-8").decode("unicode_escape")
        # step2: bytes that are now latin-1 representation of original UTF-8 -> decode to utf-8
        fixed = step1.encode("latin-1").decode("utf-8")
        return fixed
    except Exception:
        try:
            # alternative: direct latin-1 -> utf-8 decode
            return s.encode("latin-1").decode("utf-8")
        except Exception:
            return s


def normalize_text(s: str) -> str:
    """
    Clean and normalize an LLM-produced string:
    - Fix mojibake / escaped unicode (e.g. \\u00e2\\u009c\\u0094 -> ✔)
    - Replace escaped unicode sequences like '\\u2714' (literal) using unicode-escape
    - Replace repeated newlines with single blank lines
    - Trim leading/trailing whitespace on each line and collapse multiple spaces
    """
    if not s:
        return s

    # 1) Fix common mojibake (double-encoded UTF-8)
    try:
        s = _fix_mojibake(s)
    except Exception:
        pass

    # 2) If there remain literal escape sequences like '\u2714' convert them
    try:
        s2 = s.encode("utf-8").decode("unicode_escape")
        # if decoding changed something meaningful, keep it
        if s2 and s2 != s:
            s = s2
    except Exception:
        pass

    # Convert check marks to visible char (just in case)
    s = s.replace("\u2714", "✔")
    s = s.replace("\\u2714", "✔")

    # Normalize line endings & whitespace
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    s = re.sub(r"\n\s*\n+", "\n\n", s)  # collapse multiple blank lines
    s = "\n".join([ln.strip() for ln in s.split("\n")])  # trim line ends
    s = re.sub(r"[ ]{2,}", " ", s)  # collapse multiple spaces
    return s.strip()


def beautify_api_response(cleaned_payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    Take cleaned_payload (the dict you return to API) and produce a
    more human-friendly 'beautified' structure for display or Excel upload.

    Input shape:
      {
        "combined_summary": "<string>",
        "individualized_summary": [
          {"file_name": ..., "summary": ..., "document_type": ..., "processed_summary": ...},
          ...
        ]
      }

    Output shape (dict):
      {
        "combined_summary": "<normalized multi-line string>",
        "individualized_summary": [
           {
             "file_name": "<filename>",
             "summary": "<normalized single-paragraph summary>",
             "document_type": "<ACORD|Loss Run|SOV>",
             "processed_summary": "<normalized multi-line narrative>"
           },
           ...
        ]
      }
    """
    out = {"combined_summary": None, "individualized_summary": []}

    combined = cleaned_payload.get("combined_summary")
    out["combined_summary"] = normalize_text(combined) if combined else None

    indiv = cleaned_payload.get("individualized_summary", [])
    beautified_list = []
    if isinstance(indiv, Iterable):
        for item in indiv:
            file_name = item.get("file_name")
            summary = item.get("summary")
            doc_type = item.get("document_type")
            processed = item.get("processed_summary")

            beautified_item = {
                "file_name": file_name,
                # we keep 'summary' as single paragraph — collapse newlines to space
                "summary": None if not summary else " ".join(normalize_text(summary).splitlines()),
                "document_type": doc_type,
                # processed_summary may be multi-line; keep line breaks for readability
                "processed_summary": None if not processed else normalize_text(processed),
            }
            beautified_list.append(beautified_item)

    out["individualized_summary"] = beautified_list
    return out


⸻

What changed & why this will fix your output
	1.	Robust mojibake fixer (_fix_mojibake) tries to recover \u00e2\u009c\u0094 style sequences into the intended symbol (common when UTF-8 bytes were escaped). This resolves the \u00e2\u009c\u0094 you saw.
	2.	normalize_text then does unicode_escape decode fallback and trims/collapses blank lines while preserving paragraph structure.
	3.	beautify_api_response returns summary as a single paragraph (suitable for Excel cell) and processed_summary as multi-line (so it’s readable).
	4.	Summary handler prints the beautified strings (human-readable) and returns the beautified dict — free of raw text and free of escaped sequences.

⸻

Next steps you can take (suggestions)
	•	Run your test_script.py. The returned JSON should now show readable ✔ and real line breaks instead of \u00e2\u009c\u0094 sequences and escaped \n tokens.
	•	When you’re ready to write to Excel, use beautified["individualized_summary"] — each entry has:
	•	file_name (string)
	•	summary (single-line paragraph — good for an Excel cell)
	•	document_type (ACORD/Loss Run/SOV)
	•	processed_summary (multi-line narrative — you can place it in a cell)
	•	If you want, I can now:
	•	Provide the exact code snippet to write beautified["individualized_summary"] into an Excel sheet (one row per file, columns: file_name, summary, document_type, processed_summary).
	•	Or run a small local sample to show the exact JSON the API will return (using your sample).

Which would you like next — Excel write code, or run a sample transformation of the output you pasted to show the cleaned result?
