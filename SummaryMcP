summary handler.py




import os
import json
from typing import Dict, Any, List
import pandas as pd

from summary_service import summary_require_env
from summary_storage_utils import download_blob_to_input_folder
from summary_loader import load_summary_document
from summary_classifier import summarize_first_page
from Data_Filler import extract_filenames_and_summaries
from Data_Filler import connect_to_blob, beautify_extracted_data
from newsummary import summarize_attachments
from separate_summary import individual_summary
from Data_Filler import beautify_api_response

INPUT_FOLDER = "input"
OUTPUT_FOLDER = "output"


def clean_result_for_api(result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Build a compact, clean dictionary response from the full 'result' dict.

    Output shape:
    {
      "combined_summary": "<string or None>",
      "individualized_summary": [
         { "file_name": ..., "summary": ..., "document_type": ..., "processed_summary": ... },
         ...
      ]
    }

    This removes raw 'text' and error details from the per-file items.
    """
    combined = result.get("combined_summary")

    raw_individualized = result.get("individualized_summary", [])
    cleaned_list: List[Dict[str, Any]] = []

    if isinstance(raw_individualized, list):
        for item in raw_individualized:
            cleaned_item = {
                "file_name": item.get("file_name"),
                "summary": item.get("summary"),
                "document_type": item.get("document_type"),
                "processed_summary": item.get("processed_summary"),
            }
            cleaned_list.append(cleaned_item)

    return {
        "combined_summary": combined,
        "individualized_summary": cleaned_list,
    }


async def summarize_blob_pdfs_layout(blob_urls: List[str]) -> Dict[str, Any]:
    """
    For each Blob URL:
      - Download PDF into input/
      - Use Form Recognizer to extract text (all pages)
      - Use LLM to summarize the FIRST PAGE

    Saves the full detailed JSON (including raw text) to ./output/layout_summaries_result.json
    but RETURNS a cleaned dictionary suitable for API responses (no raw 'text').
    """
    try:
        summary_require_env()
    except Exception as e:
        # Return a clean, empty-shaped response on environment failure
        return {"combined_summary": None, "individualized_summary": []}

    summaries: List[Dict[str, Any]] = []

    for blob_url in blob_urls:
        file_name = None

        # 1) Download PDF
        try:
            local_pdf = await download_blob_to_input_folder(
                blob_url, input_folder=INPUT_FOLDER
            )
            file_name = os.path.basename(local_pdf)
        except Exception as e:
            summaries.append(
                {
                    "blob_url": blob_url,
                    "file_name": file_name,
                    "summary": None,
                    "text": None,
                    "error": f"Failed to download blob: {e}",
                }
            )
            continue

        # 2) Use Form Recognizer to analyze the document
        try:
            document_content = load_summary_document(local_pdf)
        except Exception as e:
            summaries.append(
                {
                    "blob_url": blob_url,
                    "file_name": file_name,
                    "summary": None,
                    "text": None,
                    "error": f"Form Recognizer failed: {e}",
                }
            )
            continue

        # Extract raw text (stored in first_page_text key by loader)
        text_content = document_content.get("first_page_text", "")

        # 3) Summarize FIRST PAGE using LLM (short summary)
        try:
            summary_text = summarize_first_page(document_content, file_name=file_name)
        except Exception as e:
            summaries.append(
                {
                    "blob_url": blob_url,
                    "file_name": file_name,
                    "summary": None,
                    "text": text_content,
                    "error": f"Summarization failed: {e}",
                }
            )
            continue

        # 4) Success case for this document — include raw text for audit on disk
        summaries.append(
            {
                "blob_url": blob_url,
                "file_name": file_name,
                "summary": summary_text,
                "text": text_content,
                "error": None,
            }
        )

    result: Dict[str, Any] = {"status": True, "summaries": summaries}

    # 5) Save result to ./output (retain 'text' on disk for audit)
    try:
        os.makedirs(OUTPUT_FOLDER, exist_ok=True)
        out_path = os.path.join(OUTPUT_FOLDER, "layout_summaries_result.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
    except Exception as e:
        result["save_error"] = str(e)

    # 6) Beautify and optionally write to Excel (unchanged; uses only file_name + summary)
    try:
        extracted_data = extract_filenames_and_summaries(OUTPUT_FOLDER)
        print("Beautified Extracted Data:\n")
        beautify_data = beautify_extracted_data(extracted_data)
        print(beautify_data)

        DATA_URL = os.getenv("Data_URL")
        if DATA_URL:
            try:
                updated_data = connect_to_blob(DATA_URL, beautify_data)
                print("Blob Data Processed Successfully.")
                if isinstance(updated_data, pd.DataFrame):
                    print("Blob Data Processed Successfully and Excel Updated.")
                else:
                    # updated_data may be an error dict
                    print(f"Error: {updated_data.get('error')}")
            except Exception as e:
                print(f"Error processing blob data: {e}")
        else:
            print("Data_URL environment variable is not set.")
    except Exception as e:
        print(f"Error in beautify / blob step: {e}")

    # 7) Combined submission summary (uses file_name + summary only)
    combined_summary_text = None
    filenamesummary_extracted_beautified: List[Dict[str, Any]] = []
    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        output_folder = "output"

        json_path = os.path.join(base_dir, output_folder, "layout_summaries_result.json")
        with open(json_path, "r", encoding="utf-8") as f:
            layout_json = json.load(f)

        filenamesummary_extracted_beautified = extract_filenames_and_summaries(output_folder)
        print("Per-document summaries (with text):")
        print(filenamesummary_extracted_beautified)

        combined_summary_text = summarize_attachments(filenamesummary_extracted_beautified)
        print("Combined Submission Summary:")
        print(combined_summary_text)

        result["combined_summary"] = combined_summary_text
    except Exception as e:
        print(f"Error generating combined submission summary: {e}")
        result["combined_summary_error"] = str(e)
        combined_summary_text = None

    # 8) Individual typed summaries (ACORD / Loss Run / SOV)
    try:
        # individual_summary expects attachments each having 'file_name','summary','text'
        individualized_summary = individual_summary(filenamesummary_extracted_beautified)
        print("Individualized Summary Output (raw):")
        print(individualized_summary)
        result["individualized_summary"] = individualized_summary
    except Exception as e:
        print(f"Error generating Individual summary: {e}")
        result["individualized_summary_error"] = str(e)
        result["individualized_summary"] = []


    # 9) CLEAN the result for the API and return only the compact payload (no raw text)
    cleaned_payload = clean_result_for_api(result)


    
    try:
        beautified = beautify_api_response(cleaned_payload)
        print(beautified)
        # print("\n----- COMBINED SUBMISSION SUMMARY (beautified) -----\n")
        # if beautified["combined_summary"]:
        #     print(beautified["combined_summary"])
        # else:
        #     print("No combined summary available.")

    #     DATA_URL = os.getenv("Data_URL")
    #     if DATA_URL:
    #         try:
    #             updated_data = connect_to_blob(DATA_URL, beautify_data)
    #             print("Blob Data Processed Successfully.")
    #             if isinstance(updated_data, pd.DataFrame):
    #                 print("Blob Data Processed Successfully and Excel Updated.")
    #             else:
    #                 # updated_data may be an error dict
    #                 print(f"Error: {updated_data.get('error')}")
    #         except Exception as e:
    #             print(f"Error processing blob data: {e}")
    #     else:
    #         print("Data_URL environment variable is not set.")
    except Exception as e:
        print(f"Error in beautify / blob step: {e}")



    # return cleaned_payload
    return beautified




Data filler.py



import os
import json
import pandas as pd
from io import BytesIO
from azure.storage.blob import BlobServiceClient
from typing import List, Dict, Any
from summary_service import summary_require_env
from summary_service import AZURE_STORAGE_CONNECTION_STRING, _parse_blob_url
import re
from typing import Iterable


def extract_filenames_and_summaries(output_folder: str) -> List[Dict[str, Any]]:
    """
    Extracts file names, summaries, and raw text from the JSON file in the output folder.

    Returns list of:
      {
        "file_name": "<file_name>",
        "summary": "<summary>",
        "text": "<full_text_or_first_page_text>"
      }
    """
    result_file = os.path.join(output_folder, "layout_summaries_result.json")

    if not os.path.exists(result_file):
        raise FileNotFoundError(f"No JSON result file found at {result_file}")

    with open(result_file, "r", encoding="utf-8") as f:
        data = json.load(f) 

    summaries = data.get("summaries", [])

    extracted_data = [
        {
            "file_name": item["file_name"],
            "summary": item["summary"],
            "text": item.get("text", "")  # <-- thisismyNEW: raw text stored here
        }
        for item in summaries
        if "file_name" in item and "summary" in item
    ]

    return extracted_data

#Absolute working code 
def beautify_extracted_data(extracted_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Beautifies and prints the extracted data in the desired format.
    NOTE: we do NOT include 'text' here, only file_name + summary + reference_number.
    """
    beautified_data: List[Dict[str, Any]] = []

    for item in extracted_data:
        file_name = item["file_name"]
        reference_number = file_name.split("_")[0]

        beautified_item = {
            "reference_number": reference_number,
            "file_name": file_name,
            "summary": item["summary"],
        }
        beautified_data.append(beautified_item)

        print(f"file_name: {file_name}")
        print(f"reference_number: {reference_number}")
        print(f"summary: {item['summary']}\n")

    return beautified_data



def connect_to_blob(blob_url: str, beautified_data: List[Dict[str, Any]]):
    """
    Update existing rows in the Excel blob by concatenating file_name + summary
    into the Summary cell for matching 'Unique refrence Number'.
    Does NOT create new rows if there is no match.
    """
    try:
        summary_require_env()
    except Exception as e:
        return {"status": False, "error": f"Environment misconfigured: {e}"}

    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("AZURE_STORAGE_CONNECTION_STRING not set")

    try:
        container, blob_path = _parse_blob_url(blob_url)
    except Exception as e:
        return {"status": False, "error": f"Unable to parse blob URL: {e}"}

    try:
        blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)
        blob_client = blob_service_client.get_blob_client(container=container, blob=blob_path)

        blob_data = blob_client.download_blob().readall()
        excel_data = pd.read_excel(BytesIO(blob_data))

        if "Unique refrence Number" not in excel_data.columns:
            return {"status": False, "error": "'Unique refrence Number' column is missing in the Excel file."}

        if "Summary" not in excel_data.columns:
            excel_data["Summary"] = None

        for item in beautified_data:
            reference_number = item.get("reference_number")
            file_name = item.get("file_name", "")
            summary_text = item.get("summary", "")

            matches = excel_data[excel_data["Unique refrence Number"] == reference_number].index

            if matches.empty:
                print(f"Reference number not found in Excel, skipping: {reference_number}")
                continue

            new_fragment = f"file_name: {file_name}\nsummary: {summary_text}"

            for idx in matches:
                existing = excel_data.at[idx, "Summary"]
                if existing is not None and not pd.isna(existing):
                    updated = f"{existing}\n\n{new_fragment}"
                else:
                    updated = new_fragment
                excel_data.at[idx, "Summary"] = updated

        output_stream = BytesIO()
        excel_data.to_excel(output_stream, index=False, engine="openpyxl")
        output_stream.seek(0)
        blob_client.upload_blob(output_stream, overwrite=True)

        return excel_data

    except Exception as e:
        return {"status": False, "error": f"Failed to process blob: {e}"}






def normalize_text(s: str) -> str:
    """
    Clean and normalize an LLM-produced string:
    - Replace escaped unicode sequences like '\\u2714' with '✔' (or remove)
    - Replace repeated newlines with single newline
    - Trim leading/trailing whitespace on each line and collapse multiple spaces
    - Remove stray carriage returns
    """
    if not s:
        return s
    # if the string contains literal backslash-u (e.g. "\\u2714") turn into the actual char
    try:
        # decode unicode-escapes (only when safe) — but avoid double-decoding valid unicode
        # This will transform "\\u2714" into "✔"
        s = s.encode("utf-8").decode("unicode_escape")
    except Exception:
        # fallback: ignore decoding errors
        pass

    # Convert any ✔ variants (some LLMs produce unicode check) to a single visible symbol
    s = s.replace("\u2714", "✔")
    s = s.replace("\\u2714", "✔")  # literal backslash-u

    # Normalize line endings & whitespace
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    # collapse multiple blank lines to single
    s = re.sub(r"\n\s*\n+", "\n\n", s)
    # trim trailing spaces on each line
    s = "\n".join([ln.strip() for ln in s.split("\n")])
    # collapse more-than-two spaces
    s = re.sub(r"[ ]{2,}", " ", s)
    # final strip
    return s.strip()




def beautify_api_response(cleaned_payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    Take cleaned_payload (the dict you return to API) and produce a
    more human-friendly 'beautified' structure for display or Excel upload.

    Input shape:
      {
        "combined_summary": "<string>",
        "individualized_summary": [
          {"file_name": ..., "summary": ..., "document_type": ..., "processed_summary": ...},
          ...
        ]
      }

    Output shape (dict):
      {
        "combined_summary": "<normalized multi-line string>",
        "individualized_summary": [
           {
             "file_name": "<filename>",
             "summary": "<normalized single-paragraph summary>",
             "document_type": "<ACORD|Loss Run|SOV>",
             "processed_summary": "<normalized multi-line narrative>"
           },
           ...
        ]
      }

    This function **does not** return 'text' or raw LLM outputs with \n escapes.
    """
    out = {"combined_summary": None, "individualized_summary": []}

    combined = cleaned_payload.get("combined_summary")
    out["combined_summary"] = normalize_text(combined) if combined else None

    indiv = cleaned_payload.get("individualized_summary", [])
    beautified_list = []
    if isinstance(indiv, Iterable):
        for item in indiv:
            # defensive: ensure keys exist
            file_name = item.get("file_name")
            summary = item.get("summary")
            doc_type = item.get("document_type")
            processed = item.get("processed_summary")

            beautified_item = {
                "file_name": file_name,
                "summary": normalize_text(summary) if summary else None,
                "document_type": doc_type,
                "processed_summary": normalize_text(processed) if processed else None,
            }
            beautified_list.append(beautified_item)

    out["individualized_summary"] = beautified_list
    return out
