import os
from dotenv import load_dotenv
from pypdf import PdfReader

from langchain_openai import OpenAIEmbeddings, AzureChatOpenAI
from langchain_chroma import Chroma
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# -------------------------------------------------
# Load environment variables
# -------------------------------------------------
load_dotenv()

AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")

AZURE_OPENAI_CHAT_DEPLOYMENT = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")
AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT")

# -------------------------------------------------
# Read PDF
# -------------------------------------------------
def read_pdf_text(pdf_path: str) -> str:
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text() or ""
    return text

# -------------------------------------------------
# Create document chunks
# -------------------------------------------------
def create_documents(text: str):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    return splitter.create_documents([text])

# -------------------------------------------------
# Main RAG function
# -------------------------------------------------
def rag_chat(pdf_path: str):
    print("üìÑ Reading PDF...")
    text = read_pdf_text(pdf_path)

    print("‚úÇÔ∏è Splitting into chunks...")
    documents = create_documents(text)

    print("üî¢ Creating embeddings...")
    embeddings = OpenAIEmbeddings(
        deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT,
        openai_api_key=AZURE_OPENAI_API_KEY,
        openai_api_base=AZURE_OPENAI_ENDPOINT,
        openai_api_version=AZURE_OPENAI_API_VERSION,
    )

    print("üì¶ Creating / loading vector store...")
    vector_store = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory="./chroma_db"
    )
    vector_store.persist()

    print("ü§ñ Initializing chat model...")
    chat_model = AzureChatOpenAI(
        deployment_name=AZURE_OPENAI_CHAT_DEPLOYMENT,
        openai_api_key=AZURE_OPENAI_API_KEY,
        openai_api_base=AZURE_OPENAI_ENDPOINT,
        openai_api_version=AZURE_OPENAI_API_VERSION,
        temperature=0
    )

    print("üîó Creating RAG chain...")
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=chat_model,
        retriever=vector_store.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=True
    )

    chat_history = []

    print("\n‚úÖ You can now chat with your PDF (type 'exit' to quit)\n")

    while True:
        query = input("Question: ")
        if query.lower() == "exit":
            break

        result = qa_chain({
            "question": query,
            "chat_history": chat_history
        })

        answer = result["answer"]
        chat_history.append((query, answer))

        print(f"\nAnswer:\n{answer}\n")

# -------------------------------------------------
# Entry point
# -------------------------------------------------
if __name__ == "__main__":
    pdf_file_path = "output/your_downloaded_file.pdf"  # <-- change this
    rag_chat(pdf_file_path)
