main.py

import os 
from dotenv import load_dotenv
from handler import load_blob_url
from handler import blobsplitter
from handler import downloadblob
from prompt import rag_chat

load_dotenv()

if __name__ == "__main__":
    try:  
        Blob_path = os.environ.get("BLOB_URL")
        if Blob_path.startswith('"') and Blob_path.endswith('"'):
            Blob_path = Blob_path[1:-1]
    except Exception as e:
        print(f"Blob url is missing", e)
    else:
        connection_string = load_blob_url()
        names = blobsplitter(Blob_path)
        downloaded_pdf_path = downloadblob(names, connection_string)

        # Now call rag_chat with the downloaded PDF path
        rag_chat(downloaded_pdf_path)


prompt.py


import os
from dotenv import load_dotenv
from pypdf import PdfReader

from langchain_openai import OpenAIEmbeddings, AzureChatOpenAI
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# -------------------------------------------------
# Load env
# -------------------------------------------------
load_dotenv()

AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")

CHAT_DEPLOYMENT = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")
EMBED_DEPLOYMENT = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME")

print(AZURE_OPENAI_API_KEY)
print(AZURE_OPENAI_ENDPOINT)
print(AZURE_OPENAI_API_VERSION)
print(CHAT_DEPLOYMENT)
print(EMBED_DEPLOYMENT)




# -------------------------------------------------
# PDF Reader
# -------------------------------------------------
def read_pdf_text(pdf_path: str) -> str:
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text() or ""
    return text

# -------------------------------------------------
# Chunking
# -------------------------------------------------
def create_documents(text: str):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    return splitter.create_documents([text])

# -------------------------------------------------
# Main RAG
# -------------------------------------------------
def rag_chat(pdf_path: str):
    print("üìÑ Reading PDF...")
    text = read_pdf_text(pdf_path)

    print("‚úÇÔ∏è Splitting text...")
    documents = create_documents(text)

    print("üî¢ Embeddings...")
    embeddings = OpenAIEmbeddings(
        deployment=EMBED_DEPLOYMENT,
        openai_api_key=AZURE_OPENAI_API_KEY,
        openai_api_base=AZURE_OPENAI_ENDPOINT,
        openai_api_version=AZURE_OPENAI_API_VERSION,
    )

    print("üì¶ Vector store...")
    vectorstore = Chroma.from_documents(
        documents,
        embedding=embeddings,
        persist_directory="./chroma_db"
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    print("ü§ñ Chat model...")
    llm = AzureChatOpenAI(
        deployment_name=CHAT_DEPLOYMENT,
        openai_api_key=AZURE_OPENAI_API_KEY,
        openai_api_base=AZURE_OPENAI_ENDPOINT,
        openai_api_version=AZURE_OPENAI_API_VERSION,
        temperature=0
    )

    prompt = ChatPromptTemplate.from_template(
        """You are a helpful assistant.
Use the context below to answer the question.

Context:
{context}

Question:
{question}
"""
    )

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
    )

    print("\n‚úÖ Chat started (type 'exit')\n")

    while True:
        query = input("Question: ")
        if query.lower() == "exit":
            break

        response = rag_chain.invoke(query)
        print("\nAnswer:\n", response.content, "\n")

# -------------------------------------------------
# Run
# -------------------------------------------------
if __name__ == "__main__":
    rag_chat("output/your_downloaded_file.pdf")

handler.py

import os 
from service import require_env
from dotenv import load_dotenv
from service import blob_splitter, dowload_blob


load_dotenv()

def load_blob_url()->str:
    try:
        connection_string = require_env()
    except Exception as e:
        print(f"There is a error ",e)
    else:
        print("Connected to Azure storage connection string")
        return connection_string

#blob_url:list[str]

def blobsplitter(blob_url:str)->list[str]:
    try:
        names = blob_splitter(blob_url)
    except:
        raise Exception ("blob_url is missing from env")

    else:
        return names

def downloadblob(names, connectionstring):
    try:
        file_path = dowload_blob(names, connectionstring)
        return file_path
    except:
        raise Exception("unable to download blob")

service.py

import os
from dotenv import load_dotenv
from azure.storage.blob import BlobServiceClient


load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING")
AZURE_STORAGE_ACCOUNT_NAME = os.environ.get("AZURE_STORAGE_ACCOUNT_NAME")
def require_env()->str:
    """Ensures Required environment variables exists"""
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise Exception("Missing Azure_Storage_Connection_String")
    
    if not AZURE_STORAGE_ACCOUNT_NAME:
        raise Exception("Missing Azure_Storage_Account_Name")
    else:
        return AZURE_STORAGE_CONNECTION_STRING


def blob_splitter(blob_urls:str)-> list[str]:
    parts = blob_urls.split("/")
    container_name = parts[3]
    blob_name = parts[4]
    print(f"container name : {container_name}", f" Blob_name {blob_name}")
    return [container_name, blob_name]

def dowload_blob(container_blob_name, connection_string):
    print(f"This is my container_blob_name {container_blob_name}")

    container_name = container_blob_name[0]
    blob_name = container_blob_name[1]

    blob_service_client = BlobServiceClient.from_connection_string(connection_string)
    container_client = blob_service_client.get_container_client(container_name)

        # Download the blob
    blob_client = container_client.get_blob_client(blob_name)
    # download_file_path = os.path.basename(blob_name)

    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    OUTPUT_FOLDER = os.path.join(BASE_DIR, "output")

    # Create the output folder if it doesn't exist
    if not os.path.exists(OUTPUT_FOLDER):
        os.makedirs(OUTPUT_FOLDER)

    output_file_path = os.path.join(OUTPUT_FOLDER, blob_name)

    with open(output_file_path, "wb") as file:
        file.write(blob_client.download_blob().readall())

    print(f"File downloaded successfully: {output_file_path}")

    return output_file_path

Connected to Azure storage connection string
container name : output-results  Blob_name 123ASJKDB1JKBSAF_attachment_Acord_125_High_tech_solution.pdf
This is my container_blob_name ['output-results', '123ASJKDB1JKBSAF_attachment_Acord_125_High_tech_solution.pdf']
File downloaded successfully: C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\output\123ASJKDB1JKBSAF_attachment_Acord_125_High_tech_solution.pdf
üìÑ Reading PDF...
‚úÇÔ∏è Splitting text...
üî¢ Embeddings...
üì¶ Vector store...
Traceback (most recent call last):
  File "C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\main.py", line 23, in <module>
    rag_chat(downloaded_pdf_path)
  File "C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\prompt.py", line 305, in rag_chat
    vectorstore = Chroma.from_documents(
                  ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\venv\Lib\site-packages\langchain_chroma\vectorstores.py", line 1431, in from_documents
    return cls.from_texts(
           ^^^^^^^^^^^^^^^
  File "C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\venv\Lib\site-packages\langchain_chroma\vectorstores.py", line 1365, in from_texts
    chroma_collection.add_texts(
  File "C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\venv\Lib\site-packages\langchain_chroma\vectorstores.py", line 627, in add_texts
    embeddings = self._embedding_function.embed_documents(texts)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\venv\Lib\site-packages\langchain_openai\embeddings\base.py", line 709, in embed_documents
    return self._get_len_safe_embeddings(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\venv\Lib\site-packages\langchain_openai\embeddings\base.py", line 576, in _get_len_safe_embeddings
    response = self.client.create(input=batch_tokens, **client_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\venv\Lib\site-packages\openai\resources\embeddings.py", line 132, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2000137378\Desktop\newproject\API_to_llmpromt\venv\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}
(venv) PS C:\Users\2000137378\Desktop\newproject\API_to_llmpromt> 
