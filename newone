import os
import json
from dotenv import load_dotenv
from openai import AzureOpenAI

# ================== CONFIG ==================

load_dotenv()

client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2024-05-01-preview"
)

MODEL = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")

BASE_DIR = os.getcwd()
OUTPUT_DIR = os.path.join(BASE_DIR, "output")
INPUT_FILE = os.path.join(OUTPUT_DIR, "output.json")
FINAL_FILE = os.path.join(OUTPUT_DIR, "final_output.json")

os.makedirs(OUTPUT_DIR, exist_ok=True)

MAX_CHARS = 12000

# ================== UTILS ==================

def chunk_text(text, max_chars=MAX_CHARS):
    chunks, current, length = [], [], 0
    for line in text.splitlines():
        if length + len(line) > max_chars:
            chunks.append("\n".join(current))
            current, length = [], 0
        current.append(line)
        length += len(line)
    if current:
        chunks.append("\n".join(current))
    return chunks

# ================== WORD EXTRACTION ==================

def extract_words(raw_json):
    """
    Extract ONLY words from:
    analyzeResult -> pages[] -> words[]
    """

    if not isinstance(raw_json, dict):
        raise ValueError("Invalid JSON root")

    analyze_result = raw_json.get("analyzeResult")
    if not analyze_result:
        raise ValueError("Missing analyzeResult")

    pages = analyze_result.get("pages")
    if not pages:
        raise ValueError("Missing pages")

    words_dict = {}
    idx = 1

    for page in pages:
        for word in page.get("words", []):
            text = word.get("content")
            confidence = word.get("confidence")

            if text is None or confidence is None:
                continue

            words_dict[f"Word_{idx}"] = {
                "value": text,
                "confidence": confidence
            }
            idx += 1

    if not words_dict:
        raise ValueError("No words extracted")

    print(f"✅ Extracted {len(words_dict)} words")
    return words_dict

# ================== GPT ==================

def build_prompt(word_data):
    return f"""
You are a JSON formatter.

Group OCR words into meaningful insurance sections
(e.g. Header, Applicant Info, Policy Info, Contact Info, etc).

Rules:
- Output ONLY valid JSON
- Do NOT change confidence values
- Do NOT calculate averages
- Keep original word text

Input:
{json.dumps(word_data, indent=2)}
"""

def process_chunk(chunk):
    response = client.chat.completions.create(
        model=MODEL,
        temperature=0,
        messages=[
            {"role": "system", "content": "Return ONLY valid JSON."},
            {"role": "user", "content": build_prompt(chunk)}
        ]
    )

    text = response.choices[0].message.content.strip()
    start = text.find("{")
    end = text.rfind("}") + 1
    return json.loads(text[start:end])

# ================== CONFIDENCE ==================

def calculate_document_confidence(raw_json):
    def collect(obj):
        values = []
        if isinstance(obj, dict):
            for k, v in obj.items():
                if k == "confidence" and isinstance(v, (int, float)):
                    values.append(v)
                else:
                    values.extend(collect(v))
        elif isinstance(obj, list):
            for i in obj:
                values.extend(collect(i))
        return values

    all_conf = collect(raw_json)
    return sum(all_conf) / len(all_conf) if all_conf else None

# ================== PIPELINE ==================

def run_pipeline(raw_json):
    words = extract_words(raw_json)

    chunks = chunk_text(json.dumps(words, indent=2))
    final_output = {}

    for chunk in chunks:
        grouped = process_chunk(chunk)
        for k, v in grouped.items():
            if k in final_output and isinstance(v, dict):
                final_output[k].update(v)
            else:
                final_output[k] = v

    final_output["Document_Confidence_Score"] = calculate_document_confidence(raw_json)

    with open(FINAL_FILE, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=2, ensure_ascii=False)

    print(f"✅ Final output written to {FINAL_FILE}")

# ================== ENTRY ==================

if __name__ == "__main__":

    if not os.path.exists(INPUT_FILE):
        raise FileNotFoundError(f"{INPUT_FILE} not found")

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        raw_json = json.load(f)

    run_pipeline(raw_json)
