
This is the code for Data extraction agent I want it to be written in such a way that the orchestration agent triggers this data extraction agent 
as we can see in orchestration code that i shared here it triggeres the data extraction agent (   if chosen == "DataExtractionAgent":
            try:
                url = "http://127.0.0.1:8661/start-processing"
                response = requests.post(url, timeout=10)
                if response.status_code == 200:
                    print("Data Extraction Agent triggered successfully.")
                else:
                    print(f"Failed to trigger Data Extraction Agent. Status code: {response.status_code}")
            except Exception as e:
                print(f"Error triggering Data Extraction Agent: {e}")

        # elif chosen == "NONE":
        #     print("No further agents to run. Exiting orchestration loop.")
        #     break

        time.sleep(10)
   )

so let me explain how the  flow should be,
 i will go in this data extraction agent and run it in the terminal there it should give that fast api server is running and nothing should happen it should only start its fast api server
after that i will go to Orchestration agent there i will run it in the terminal and once llm suggests data extraction (as mentioned in the code) the data extraction agent should get triggered so we need a  url = "http://127.0.0.1:8661/start-processing" this api only after which the Data extraction agent should start the actual process 
so you dont need to change anything in orchestration agent , rather i think the changes are required in data extraction agent , please ask me if you have any questions

import uvicorn
from fastapi import FastAPI, UploadFile, File, BackgroundTasks
from fastapi.responses import FileResponse, JSONResponse
from dotenv import load_dotenv
import os
import pandas as pd
import tempfile
import json
from datetime import datetime
from image_processor import analyze_document
import asyncio
 
# Load environment variables
load_dotenv()
ENDPOINT = os.getenv("AZURE_FORMRECOG_ENDPOINT")
API_KEY = os.getenv("AZURE_FORMRECOG_KEY")

COMMON_DIR =  os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "Eligibilty_check_UW", "input")
# Ensure output and input directories exist
MOUNT_DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "data")
os.makedirs(MOUNT_DATA_DIR, exist_ok=True)  # Ensure the Mount_Data directory exists

# Define the path to the Extracted_Data folder inside Mount_Data
OUTPUT_DIR = os.path.join(MOUNT_DATA_DIR, "Extracted_Data")
os.makedirs(OUTPUT_DIR, exist_ok=True)  # Ensure the Extracted_Data directory exists




BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# OUTPUT_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "output")
INPUT_DIR = os.path.join(BASE_DIR, "input")
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(INPUT_DIR, exist_ok=True)
 
app = FastAPI()
 
@app.post("/extract")
async def extract_fields(image: UploadFile = File(...), excel: UploadFile = File(...), background_tasks: BackgroundTasks = None):
    try:
        image_data = await image.read()
 
        # Save and read Excel file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_excel:
            tmp_excel.write(await excel.read())
            tmp_excel_path = tmp_excel.name
 
        df = pd.read_excel(tmp_excel_path, engine="openpyxl", header=None)
 
        if df.empty or df.shape[1] < 1:
            try:
                os.remove(tmp_excel_path)
            except Exception:
                pass
            return JSONResponse(
                status_code=400,
                content={"error": "Excel file must have at least one column with fields to extract."},
            )
 
        fields = df[0].dropna().astype(str).tolist()
        categories = {"Uncategorized": fields}
 
        result = analyze_document(
            ENDPOINT, API_KEY, image_data=image_data, fields_to_extract=fields, categories=categories
        )
 
        rows = []
        fields_with_values = result.get("fields_with_values", {})
        selected_fields = set(result.get("selected_fields", []))
 
        for key, val in fields_with_values.items():
            rows.append({
                "Field": key,
                "Value": val,
                "Selected": key in selected_fields
            })
 
        for key in selected_fields:
            if key not in fields_with_values:
                rows.append({
                    "Field": key,
                    "Value": "",
                    "Selected": True
                })
 
        df_out = pd.DataFrame(rows, columns=["Field", "Value", "Selected"])
 
        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_out:
            tmp_out_path = tmp_out.name
        df_out.to_excel(tmp_out_path, index=False, engine="openpyxl")
 
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # excel_filename = f"extracted_{timestamp}.xlsx"
        # excel_filename = OUTPUT_DIR / "extracted.xlsx"

        excel_filename = "extracted.xlsx"
        # excel_filename = OUTPUT_DIR / f"extracted.xlsx"
        json_filename = f"extracted_{timestamp}.json"
 
        # excel_path = os.path.join(OUTPUT_DIR, excel_filename) - I commented
        excel_path = os.path.join(COMMON_DIR, excel_filename)
        df_out.to_excel(excel_path, index=False, engine="openpyxl")
 
        json_path = os.path.join(OUTPUT_DIR, json_filename)
        with open(json_path, "w", encoding="utf-8") as json_file:
            json.dump(rows, json_file, indent=4)
 
        test_json_path = os.path.join(OUTPUT_DIR, "data_extraction.json")
        status_payload = {"status": "completed"} if rows else {"status": "not completed"}
        with open(test_json_path, "w", encoding="utf-8") as test_file:
            json.dump(status_payload, test_file, indent=4)
 
        if background_tasks:
            background_tasks.add_task(lambda p: os.path.exists(p) and os.remove(p), tmp_out_path)
            background_tasks.add_task(lambda p: os.path.exists(p) and os.remove(p), tmp_excel_path)
        else:
            try:
                os.remove(tmp_excel_path)
            except Exception:
                pass
 
        return JSONResponse(
            status_code=200,
            content={
                "message": "Extraction successful.",
                "excel_file": excel_path,
                "json_file": json_path,
                "status_file": test_json_path
            }
        )
 
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})
 
# ------------------ Manual Folder Processing Logic ------------------
 
def find_file_pairs(input_dir):
    print("ðŸ” Scanning input folder...")
   
    # List all files in input directory
    files = os.listdir(input_dir)
   
    # Find excel files
    excel_files = [f for f in files if f.endswith('.xlsx')]
    # Find document files
    doc_files = [f for f in files if f.lower().endswith(('.pdf', '.png', '.jpg', '.jpeg'))]
   
    print(f" Found {len(excel_files)} Excel files and {len(doc_files)} document files")
   
    pairs = []
    for excel in excel_files:
        base_name = os.path.splitext(excel)[0]
       
        # Look for matching document files
        for doc in doc_files:
            doc_base = os.path.splitext(doc)[0]
           
            # Match if document name starts with excel name or vice versa
            if doc_base.startswith(base_name) or base_name.startswith(doc_base):
                pairs.append((
                    os.path.join(input_dir, doc),
                    os.path.join(input_dir, excel)
                ))
                print(f"âœ“ Matched: {doc} â†” {excel}")
                break
   
    print(f" Matched {len(pairs)} file pairs")
    return pairs
 
async def process_input_folder():
    pairs = find_file_pairs(INPUT_DIR)
    for image_path, excel_path in pairs:
        try:
            print(f"Processing: {os.path.basename(image_path)} + {os.path.basename(excel_path)}")
 
            with open(image_path, "rb") as img_file:
                image_data = img_file.read()
 
            df = pd.read_excel(excel_path, engine="openpyxl", header=None)
            if df.empty or df.shape[1] < 1:
                print(" Skipped: Excel file is empty or invalid.")
                continue
 
            fields = df[0].dropna().astype(str).tolist()
            categories = {"Uncategorized": fields}
 
            print(" Sending to Document Intelligence API...")
            result = analyze_document(
                ENDPOINT, API_KEY, image_data=image_data, fields_to_extract=fields, categories=categories
            )
 
            rows = []
            fields_with_values = result.get("fields_with_values", {})
            selected_fields = set(result.get("selected_fields", []))
 
            for key, val in fields_with_values.items():
                rows.append({"Field": key, "Value": val, "Selected": key in selected_fields})
            for key in selected_fields:
                if key not in fields_with_values:
                    rows.append({"Field": key, "Value": "", "Selected": True})
 
            # Create the original DataFrame
            df_out = pd.DataFrame(rows, columns=["Field", "Value", "Selected"])
           
            # Create a transposed version for Excel
            df_excel = pd.DataFrame({
                'Field': df_out['Field'].values,
                'Value': df_out['Value'].values,
                'Selected': df_out['Selected'].values
            })
           
            # Transpose the DataFrame and use the 'Field' column as headers
            df_excel_transposed = pd.DataFrame([df_excel['Value'].values, df_excel['Selected'].values],
                                             columns=df_excel['Field'].values)
 
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # excel_filename = f"extracted_{timestamp}.xlsx"
            excel_filename = "extracted.xlsx"
            json_filename = f"extracted_{timestamp}.json"
 
            # excel_path = os.path.join(OUTPUT_DIR, excel_filename) - I Commented and adde a new line below
            excel_path = os.path.join(COMMON_DIR, excel_filename)
            json_path = os.path.join(OUTPUT_DIR, json_filename)
            test_json_path = os.path.join(OUTPUT_DIR, "data_extraction.json")
 
            print("Saving extracted data...")
            # Save Excel file in transposed format
            df_excel_transposed.to_excel(excel_path, index=False, engine="openpyxl")
            print(f"Excel saved: {excel_filename}")
           
            # Save JSON files (keeping original format)
            with open(json_path, "w", encoding="utf-8") as json_file:
                json.dump(rows, json_file, indent=4)
            print(f"JSON saved: {json_filename}")
           
            with open(test_json_path, "w", encoding="utf-8") as test_file:
                json.dump({"status": "completed" if rows else "not completed"}, test_file, indent=4)
 
            print(" Extraction complete. Files saved to output folder.")
 
        except Exception as e:
            print(f" Error processing {image_path} and {excel_path}: {e}")
 

 
@app.on_event("startup")
async def startup_event():
    print(" FastAPI server starting...")
    asyncio.create_task(process_input_folder())



# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=env.get(Port_Number)
if __name__ == "__main__":
    # Read PORT_NUMBER from environment variables
    port = int(os.getenv("PORT_NUMBER", 8056))  # Default to 8000 if not set
    uvicorn.run(app, host="0.0.0.0", port=port)




below is orchestration code

import subprocess
import requests
import argparse
import json
import os
import time
from typing import Dict
import sys
from dotenv import load_dotenv

load_dotenv()

# OpenAI SDK
from openai import OpenAI, AzureOpenAI

# Local agents
# from agents import data_extraction_agent, eligibility_agent, risk_assessment_agent


def create_client_from_env() -> OpenAI:
    """Return an OpenAI client that's pre-configured for Azure if the
    AZURE_OPENAI_* variables are set; otherwise return a default client that
    talks to api.openai.com.
    """
    azure_key = os.getenv("AZURE_OPENAI_API_KEY")
    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    azure_version = os.getenv("AZURE_OPENAI_API_VERSION")
    deployment = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")

    if azure_key and azure_endpoint and azure_version and deployment:
        client: OpenAI = AzureOpenAI(
            api_key=azure_key,
            azure_endpoint=azure_endpoint,
            api_version=azure_version,
        )
        client._is_azure = True
        client._azure_deployment = deployment
        return client

    if not os.getenv("OPENAI_API_KEY"):
        raise RuntimeError(
            "OPENAI_API_KEY must be set for OpenAI-platform mode "
            "or provide all AZURE_OPENAI_* variables for Azure mode."
        )

    client = OpenAI()
    client._is_azure = False
    client._azure_deployment = None
    return client


# Create the client once, globally
client = create_client_from_env()

ALLOWED_AGENTS = ["RiskAssessmentAgent", "DataExtractionAgent", "EligibilityCheckAgent", "NONE"]

PROMPT_TEMPLATE = (
    "You are an orchestration-decider."
    "Available agents (do NOT create new ones):"
    "1) RiskAssessmentAgent â€“ reads eligibility.json, writes risk_scores.json."
    "2) DataExtractionAgent â€“ writes data_extraction.json."
    "3) EligibilityCheckAgent â€“ reads data_extraction.json, writes eligibility.json."
    "Your task is to decide which agent should run next, based on the current state of the files and their contents. "
    "Evaluate the outcome of each agent's operation as reflected in the file contents and ensure that the operation is valid and complete before proceeding to the next agent."
    "Important considerations:"
    "1) Each agent's operation depends on the validity and completeness of the previous agent's output. If an agent's output indicates that further processing is not required or possible, you must stop and not proceed to the next agent."
    "2) If an agent's output is incomplete, invalid, or indicates that further processing should not occur, provide a reason for why the next agent cannot be activated and stop further processing."
    "3) If an agent's output is complete and valid, proceed to the next agent."
    "Input: a JSON object called 'state' listing files in the data folder and their contents."
    "Reply with ONE JSON object, schema:"
    '{ "next_agent": "RiskAssessmentAgent|DataExtractionAgent|EligibilityCheckAgent|NONE", '
    '"reason": "short explanation" }'
)


def ask_llm_for_next_agent(state: Dict, model: str = "gpt-4o") -> Dict:
    """Query the LLM and return its JSON choice."""
    system_msg = {"role": "system", "content": PROMPT_TEMPLATE}
    user_msg = {"role": "user", "content": json.dumps({"state": state}, indent=2)}

    # Choose correct model / deployment
    if getattr(client, "_is_azure", False):
        model_to_call = client._azure_deployment
    else:
        model_to_call = model

    try:
        resp = client.chat.completions.create(
            model=model_to_call,
            messages=[system_msg, user_msg],
            temperature=0.0,
            max_tokens=300
        )
        print(resp)
    except Exception as e:
        raise RuntimeError(f"LLM call failed: {e}")

    try:
        text = resp.choices[0].message.content.strip()
    except Exception:
        text = str(resp)

    try:
        start, end = text.find("{"), text.rfind("}")
        parsed = json.loads(text[start:end + 1])
    except Exception as e:
        raise RuntimeError(f"LLM did not return valid JSON. Raw: {text} Error: {e}")

    return parsed


def run_orchestration(data_dir: str, model: str = "gpt-4") -> None:
    """Main loop."""
    state = {"data_dir": data_dir}
    os.makedirs(data_dir, exist_ok=True)

    # Define the extracted data folder path
    extracted_data_dir = os.path.join(data_dir, "Extracted_Data")
    os.makedirs(extracted_data_dir, exist_ok=True)

    print("Starting orchestration loop. Data folder:", data_dir)

    # Track the number of times the same agent is suggested
    consecutive_agent_suggestions = 0
    last_suggested_agent = None

    while True:
        # Get the current state of files in both data_dir and extracted_data_dir
        try:
            files_now = os.listdir(data_dir)
        except Exception:
            files_now = []
        try:
            extracted_files_now = os.listdir(extracted_data_dir)
        except Exception:
            extracted_files_now = []

        llm_state = {"files": files_now, "extracted_files": extracted_files_now}

        # Read the contents of each file in data_dir and add to state
        for file in files_now:
            full = os.path.join(data_dir, file)
            try:
                with open(full, "r", encoding="utf-8") as f:
                    llm_state[file] = json.load(f)
            except Exception:
                llm_state[file] = None  # If file is unreadable, set its content to None

        # Read the contents of each file in extracted_data_dir and add to state
        for file in extracted_files_now:
            full = os.path.join(extracted_data_dir, file)
            try:
                with open(full, "r", encoding="utf-8") as f:
                    llm_state[file] = json.load(f)
            except Exception:
                llm_state[file] = None  # If file is unreadable, set its content to None

        # Ask the LLM for the next agent
        try:
            parsed = ask_llm_for_next_agent(llm_state, model=model)
        except Exception as e:
            print("LLM call failed â€“ falling back to rule-based choice.", e)
            parsed = {"next_agent": "NONE", "reason": "llm-failed"}

        chosen = parsed.get("next_agent")
        reason = parsed.get("reason", "No reason provided.")
        print(f"LLM suggested: {chosen} | Reason: {reason}")

        # Check for repeated agent suggestions
        if chosen == last_suggested_agent:
            consecutive_agent_suggestions += 1
        else:
            consecutive_agent_suggestions = 0
        last_suggested_agent = chosen

        if consecutive_agent_suggestions > 5:
            print("The same agent has been suggested multiple times without progress. Exiting.")
            break

        if chosen == "DataExtractionAgent":
            try:
                url = "http://127.0.0.1:8661/start-processing"
                response = requests.post(url, timeout=10)
                if response.status_code == 200:
                    print("Data Extraction Agent triggered successfully.")
                else:
                    print(f"Failed to trigger Data Extraction Agent. Status code: {response.status_code}")
            except Exception as e:
                print(f"Error triggering Data Extraction Agent: {e}")

        # elif chosen == "NONE":
        #     print("No further agents to run. Exiting orchestration loop.")
        #     break

        time.sleep(10)

        if chosen == "EligibilityCheckAgent":
            try:
                url = "http://127.0.0.1:8022/start-processing"
                response = requests.post(url, timeout=10)
                if response.status_code == 200:
                    print("Eligibility Agent triggered successfully.")
                else:
                    print(f"Failed to trigger Eligibility Agent. Status code: {response.status_code}")
            except Exception as e:
                print(f"Error triggering Eligibility Agent: {e}")

            time.sleep(10)


        if chosen == "RiskAssessmentAgent":
            try:
                url = "http://127.0.0.1:8677/risk/run"
                response = requests.post(url, timeout=10)
                if response.status_code == 200:
                    print("Risk Assesment Agent triggered successfully.")
                else:
                    print(f"Failed to trigger Risk Assesment Agent. Status code: {response.status_code}")
            except Exception as e:
                print(f"Error triggering Risk Assesment Agent: {e}")

            time.sleep(10)   



        elif chosen == "NONE":
            print("No further agents to run. Exiting orchestration loop.")
            break

        time.sleep(5)

        



#-----------------------------------------------------------------------------------------------------------------------------------




if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run orchestration agent (demo)")
    parser.add_argument("--data-dir", default="./data",
                        help="Folder used to read/write JSON files")
    parser.add_argument("--model", default="gpt-4o",
                        help="Model name for OpenAI-platform mode (ignored for Azure)")
    args = parser.parse_args()

    if getattr(client, "_is_azure", False):
        print("Using Azure OpenAI deployment:", client._azure_deployment)
    else:
        if not os.getenv("OPENAI_API_KEY"):
            print("Warning: OPENAI_API_KEY not set â€“ first LLM call will fail.")

    run_orchestration(args.data_dir, model=args.model)

    




























