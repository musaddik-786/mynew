Now i have 3 agents 
emailintentagent (which has multiple tools)
eligibilityagent ((which has multiple tools)
riskagent ((which has multiple tools)

i have a big project and in that there are folders named langgrpah_client.py in each agent 

emaiintent agent
import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AIMessage, ToolMessage, SystemMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode
import os


class State(TypedDict):
        messages: Annotated[list, add_messages]


async def get_tool_list(config_mcp_server):
        client = MultiServerMCPClient(config_mcp_server)
        tools_list = await client.get_tools()
        print("Tools fetched from MCP:", [tool.name for tool in tools_list])
        return tools_list
    
config_mcp_server = {
        "email_reader_mcp":{
            "url":"http://0.0.0.0:8654/api/v1/email_intent_agent/mcp",
            "transport":"streamable_http",
        }}


def router(state: State):
    last_message = state["messages"][-1]
    if isinstance(last_message, AIMessage) and getattr(last_message, 'tool_calls', None):
        return "tools"
    if isinstance(last_message, AIMessage) and last_message.content:
        content = last_message.content
        if ('Continue' in content) :
            return "tools"
        elif ("End" in content):
            return "End"
    return "End"

def load_prompt(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def create_custom_graph(model, tools, prompt, checkpointer=None):
    #format_instruction, response_class = response_format
    graph_builder = StateGraph(State)

    llm_with_tools = model.bind_tools(tools)

    async def agent_node(state: State):
        messages = state["messages"]
        system_prompt = SystemMessage(content=prompt)
        all_messages = [system_prompt] + messages
        message = await llm_with_tools.ainvoke(all_messages)
        return {"messages": [message]}


    
    graph_builder.add_node("agent", agent_node)
    graph_builder.add_node("tools", ToolNode(tools=tools))
    

    graph_builder.add_edge(START, "agent")
    graph_builder.add_conditional_edges(
        "agent", 
        router, 
        {
            "tools": "tools",
            "End": END
        })
    graph_builder.add_edge("tools", "agent")
    

    return graph_builder.compile(checkpointer=checkpointer) if checkpointer else graph_builder.compile()

async def main():
#async def email_intent_agent():
    tools = await get_tool_list(config_mcp_server=config_mcp_server)
    

    
    from dotenv import load_dotenv
    load_dotenv()

    from langchain_openai.chat_models import AzureChatOpenAI

    endpoint = os.environ["AZURE_INFERENCE_ENDPOINT"] = os.getenv("AZURE_OPENAI_ENDPOINT")
    credential = os.environ["AZURE_INFERENCE_CREDENTIAL"] = os.getenv("AZURE_OPENAI_API_KEY")
    model = os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"] = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")

    model = AzureChatOpenAI(
        api_key="examplekey",
        api_version="2024-12-01-preview",
        azure_deployment="gpt-4",
        azure_endpoint="https://jarvisoai.openai.azure.com"
    )

    

    llm_with_tools = model.bind_tools(tools)

    SYSTEM_INSTRUCTION = load_prompt("./prompt2.txt")

    graph = create_custom_graph(model=model, tools=tools, prompt=SYSTEM_INSTRUCTION)

    
    last_message = None
    async for chunk in graph.astream({"messages": ["Start your task"]},
                                    config={"recursion_limit": 100}):

        # If the agent responded
        if "agent" in chunk:
            messages = chunk["agent"]["messages"]
            for msg in messages:
                if isinstance(msg, AIMessage):
                    # Print tool calls if present
                    if getattr(msg, "tool_calls", None):
                        print("[Agent] (tool call issued)\n")
                        for call in msg.tool_calls:
                            print(f"   → Tool name: {call['name']}")
                            print(f"   → Arguments: {call['args']}\n***********************************************")
                    # Otherwise print normal content
                    elif msg.content:
                        print(f"[Agent] {msg.content}\n***********************************************")
                    else:
                        print("[Agent] (empty)\n***********************************************")
                else:
                    print(f"[Agent] {msg}\n***********************************************")

        # If a tool responded
        if "tools" in chunk:
            messages = chunk["tools"]["messages"]
            for msg in messages:
                if isinstance(msg, ToolMessage):
                    print(f"[Tool:{msg.name}] {msg.content}\n***********************************************")
                else:
                    print(f"[Tool] {msg}\n***********************************************")



if __name__ == "__main__":
    asyncio.run(main())

eligibility agent

import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AIMessage, ToolMessage, SystemMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode
import os


class State(TypedDict):
        messages: Annotated[list, add_messages]


async def get_tool_list(config_mcp_server):
        client = MultiServerMCPClient(config_mcp_server)
        tools_list = await client.get_tools()
        print("Tools fetched from MCP:", [tool.name for tool in tools_list])
        return tools_list
    
config_mcp_server = {
        "email_reader_mcp":{
            "url":"http://0.0.0.0:8502/api/v1/email_intent_agent/mcp",
            "transport":"streamable_http",
        }}


def router(state: State):
    last_message = state["messages"][-1]
    if isinstance(last_message, AIMessage) and getattr(last_message, 'tool_calls', None):
        return "tools"
    if isinstance(last_message, AIMessage) and last_message.content:
        content = last_message.content
        if ('Continue' in content) :
            return "tools"
        elif ("End" in content):
            return "End"
    return "End"

def load_prompt(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def create_custom_graph(model, tools, prompt, checkpointer=None):
    #format_instruction, response_class = response_format
    graph_builder = StateGraph(State)

    llm_with_tools = model.bind_tools(tools)

    async def agent_node(state: State):
        messages = state["messages"]
        system_prompt = SystemMessage(content=prompt)
        all_messages = [system_prompt] + messages
       m essage = await llm_with_tools.ainvoke(all_messages)
        return {"messages": [message]}


    
    graph_builder.add_node("agent", agent_node)
    graph_builder.add_node("tools", ToolNode(tools=tools))
    

    graph_builder.add_edge(START, "agent")
    graph_builder.add_conditional_edges(
        "agent", 
        router, 
        {
            "tools": "tools",
            "End": END
        })
    graph_builder.add_edge("tools", "agent")
    

    return graph_builder.compile(checkpointer=checkpointer) if checkpointer else graph_builder.compile()

async def main():
#async def eligibility_agent():
    tools = await get_tool_list(config_mcp_server=config_mcp_server)
    

    
    from dotenv import load_dotenv
    load_dotenv()

    from langchain_openai.chat_models import AzureChatOpenAI

    endpoint = os.environ["AZURE_INFERENCE_ENDPOINT"] = os.getenv("AZURE_OPENAI_ENDPOINT")
    credential = os.environ["AZURE_INFERENCE_CREDENTIAL"] = os.getenv("AZURE_OPENAI_API_KEY")
    model = os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"] = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")

    model = AzureChatOpenAI(
        api_key="examplekey",
        api_version="2024-12-01-preview",
        azure_deployment="gpt-4",
        azure_endpoint="https://jarvisoai.openai.azure.com"
    )

    

    llm_with_tools = model.bind_tools(tools)

    SYSTEM_INSTRUCTION = load_prompt("./prompt2.txt")

    graph = create_custom_graph(model=model, tools=tools, prompt=SYSTEM_INSTRUCTION)

    
    last_message = None
    async for chunk in graph.astream({"messages": ["Start your task"]},
                                    config={"recursion_limit": 25}):

        # If the agent responded
        if "agent" in chunk:
            messages = chunk["agent"]["messages"]
            for msg in messages:
                if isinstance(msg, AIMessage):
                    # Print tool calls if present
                    if getattr(msg, "tool_calls", None):
                        print("[Agent] (tool call issued)\n")
                        for call in msg.tool_calls:
                            print(f"   → Tool name: {call['name']}")
                            print(f"   → Arguments: {call['args']}\n***********************************************")
                    # Otherwise print normal content
                    elif msg.content:
                        print(f"[Agent] {msg.content}\n***********************************************")
                    else:
                        print("[Agent] (empty)\n***********************************************")
                else:
                    print(f"[Agent] {msg}\n***********************************************")

        # If a tool responded
        if "tools" in chunk:
            messages = chunk["tools"]["messages"]
            for msg in messages:
                if isinstance(msg, ToolMessage):
                    print(f"[Tool:{msg.name}] {msg.content}\n***********************************************")
                else:
                    print(f"[Tool] {msg}\n***********************************************")



if __name__ == "__main__":
    asyncio.run(main())


Risk agent

import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AIMessage, ToolMessage, SystemMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode
import os


class State(TypedDict):
        messages: Annotated[list, add_messages]


async def get_tool_list(config_mcp_server):
        client = MultiServerMCPClient(config_mcp_server)
        tools_list = await client.get_tools()
        print("Tools fetched from MCP:", [tool.name for tool in tools_list])
        return tools_list
    
config_mcp_server = {
        "email_reader_mcp":{
            "url":"http://0.0.0.0:8502/api/v1/email_intent_agent/mcp",
            "transport":"streamable_http",
        }}


def router(state: State):
    last_message = state["messages"][-1]
    if isinstance(last_message, AIMessage) and getattr(last_message, 'tool_calls', None):
        return "tools"
    if isinstance(last_message, AIMessage) and last_message.content:
        content = last_message.content
        if ('Continue' in content) :
            return "tools"
        elif ("End" in content):
            return "End"
    return "End"

def load_prompt(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def create_custom_graph(model, tools, prompt, checkpointer=None):
    #format_instruction, response_class = response_format
    graph_builder = StateGraph(State)

    llm_with_tools = model.bind_tools(tools)

    async def agent_node(state: State):
        messages = state["messages"]
        system_prompt = SystemMessage(content=prompt)
        all_messages = [system_prompt] + messages
        message = await llm_with_tools.ainvoke(all_messages)
        return {"messages": [message]}


    
    graph_builder.add_node("agent", agent_node)
    graph_builder.add_node("tools", ToolNode(tools=tools))
    

    graph_builder.add_edge(START, "agent")
    graph_builder.add_conditional_edges(
        "agent", 
        router, 
        {
            "tools": "tools",
            "End": END
        })
    graph_builder.add_edge("tools", "agent")
    

    return graph_builder.compile(checkpointer=checkpointer) if checkpointer else graph_builder.compile()

async def main():
#async def email_intent_agent():
    tools = await get_tool_list(config_mcp_server=config_mcp_server)
    

    
    from dotenv import load_dotenv
    load_dotenv()

    from langchain_openai.chat_models import AzureChatOpenAI

    endpoint = os.environ["AZURE_INFERENCE_ENDPOINT"] = os.getenv("AZURE_OPENAI_ENDPOINT")
    credential = os.environ["AZURE_INFERENCE_CREDENTIAL"] = os.getenv("AZURE_OPENAI_API_KEY")
    model = os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"] = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")

    model = AzureChatOpenAI(
        api_key="examplekey",
        api_version="2024-12-01-preview",
        azure_deployment="gpt-4",
        azure_endpoint="https://jarvisoai.openai.azure.com"
    )

    

    llm_with_tools = model.bind_tools(tools)

    SYSTEM_INSTRUCTION = load_prompt("./prompt2.txt")

    graph = create_custom_graph(model=model, tools=tools, prompt=SYSTEM_INSTRUCTION)

    
    last_message = None
    async for chunk in graph.astream({"messages": ["Start your task"]},
                                    config={"recursion_limit": 25}):

        # If the agent responded
        if "agent" in chunk:
            messages = chunk["agent"]["messages"]
            for msg in messages:
                if isinstance(msg, AIMessage):
                    # Print tool calls if present
                    if getattr(msg, "tool_calls", None):
                        print("[Agent] (tool call issued)\n")
                        for call in msg.tool_calls:
                            print(f"   → Tool name: {call['name']}")
                            print(f"   → Arguments: {call['args']}\n***********************************************")
                    # Otherwise print normal content
                    elif msg.content:
                        print(f"[Agent] {msg.content}\n***********************************************")
                    else:
                        print("[Agent] (empty)\n***********************************************")
                else:
                    print(f"[Agent] {msg}\n***********************************************")

        # If a tool responded
        if "tools" in chunk:
            messages = chunk["tools"]["messages"]
            for msg in messages:
                if isinstance(msg, ToolMessage):
                    print(f"[Tool:{msg.name}] {msg.content}\n***********************************************")
                else:
                    print(f"[Tool] {msg}\n***********************************************")



if __name__ == "__main__":
    asyncio.run(main())
